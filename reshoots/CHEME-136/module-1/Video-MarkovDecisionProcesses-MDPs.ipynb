{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89bffcdb-8f81-43bc-8fe1-996dc806513e",
   "metadata": {},
   "source": [
    "## Markov Decision Processes (MDPs)\n",
    "A Markov decision process (MDP) models decision-making in situations where outcomes are partly random and partly under the control of a decision-maker. An MDP consists of the tuple of components $\\left(\\mathcal{S}, \\mathcal{A}, R_{a}\\left(s, s^{\\prime}\\right), T_{a}\\left(s,s^{\\prime}\\right), \\gamma\\right)$:\n",
    "\n",
    "### Components of an MDP\n",
    "* The state space $\\mathcal{S}$ is the set of all possible states $s\\in\\mathcal{S}$ that a system can exist in. This is the same idea as a Markov model. For example, let's suppose we define our state space as the set of investor moods $\\mathcal{S} \\equiv \\left\\{\\text{bullish},\\text{neutral},\\text{bearish}\\right\\}$.\n",
    "* The action space $\\mathcal{A}$ is the set of all possible actions $a\\in\\mathcal{A}$ available to the agent, where $\\mathcal{A}_{s} \\subseteq \\mathcal{A}$ is the subset of the action space $\\mathcal{A}$ that is accessible from state $s$. In our investor example, the action space could be defined as $\\mathcal{A} \\equiv \\left\\{\\text{buy},\\text{hold},\\text{sell}\\right\\}$.\n",
    "* A reward $R_{a}\\left(s, s^{\\prime}\\right)$ is received after transitioning from $s\\rightarrow{s}^{\\prime}$ due to action $a$. For example, this could be the proceeds (or losses) from the sale of shares of asset `XYZ.`\n",
    "* The state transition model $T_{a}\\left(s,s^{\\prime}\\right) = P(s_{t+1} = s^{\\prime}~|~s_{t}=s,a_{t} = a)$ denotes the probability that action $a$ in state $s$ at time $t$ will result in state $s^{\\prime}$ at time $t+1$. This idea is similar to a Markov model, e.g., it has the Markov property but involves both the current state $s$ and action $a$ as conditions to transition to the next state.\n",
    "* The discount factor $0<\\gamma<1$ weighs the future expected utility of choices. The discount factor $\\gamma$ is a hyper-parameter of various approaches used to model the decision.\n",
    "\n",
    "Finally, a policy function $\\pi:\\mathcal{S}\\rightarrow\\mathcal{A}$ is the mapping from states $s\\in\\mathcal{S}$ to actions $a\\in\\mathcal{A}$ used by the agent to solve a decision task. Ultimately, we want to develop an optimal policy function (one that gives us the best possible decisions). There are a large number of techniques to develop optimal policy functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6ef518-0295-4a50-b286-a79aa188ec1b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.1",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
