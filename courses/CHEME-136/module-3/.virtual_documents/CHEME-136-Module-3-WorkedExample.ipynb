





include("Include.jl");








rule_index = 1635;
number_of_colors = 3; # each color will be mapped to a state
radius = 4; # the next state depends upon the radius number of cells centered on the next state





number_of_states = range(0, stop = (number_of_colors - 1), step = (1/radius)) |> length;
total_number_policies = [(number_of_colors - 1)*(number_of_colors^(i-1)) for i ∈ 1:number_of_states] |> sum
println("For $(number_of_colors) colors with a radius = $(radius), the number of totalistic rules: $(total_number_policies + 1)")


policy_model = build(MyTwoDimensionalTotalisticWolframRuleModel, (
    index = rule_index,
    colors = number_of_colors,
    radius = radius,
));


number_of_states = policy_model.rule |> length


policy_model.rule





# parameters -
number_of_rows = 2^9;
number_of_columns = 2^9;
number_of_agents = number_of_rows*number_of_columns;
middle = (number_of_rows / 2) |> Int
number_of_iterations = 2^10;
view_width = 50;





# build an initial array - the initial state of the world 
initialstate = Array{Int64,2}(undef, number_of_rows, number_of_columns);
for row ∈ 1:number_of_rows
    for column ∈ 1:number_of_columns
        if (row == 1 && column == number_of_columns)
            initialstate[row, column] = 0;
        elseif (row == middle && column == middle)
            initialstate[row, column] = 0;
        elseif (row == number_of_rows && column == 1)
            initialstate[row, column] = 0;
        elseif (row == number_of_rows && column == number_of_columns)
            initialstate[row, column] = 0;
        elseif (row == 1 && column == 1)
            initialstate[row, column] = 0;
        elseif (row == middle && column == 1)
            initialstate[row, column] = 0;
        elseif (row == 1 && column == middle)
            initialstate[row, column] = 0;
        elseif (row == middle && column == number_of_columns)
            initialstate[row, column] = 0;
        elseif (row == number_of_rows && column == middle)
            initialstate[row, column] = 0;
        else
            initialstate[row, column] = 1;
        end
    end
end





frames = solve(policy_model, initialstate, steps = number_of_iterations);





turn_index_to_view = 600;





myframe = frames[turn_index_to_view]





let
    new_frame = Array{Float64,2}(undef, number_of_rows, number_of_columns);
    for j ∈ 1:number_of_rows
        for k ∈ 1:number_of_columns
            if myframe[j,k] == 0
                new_frame[j,k] = 1.0; # buy
            elseif myframe[j,k] == 1
                new_frame[j,k] = 0.5; # hold
            elseif myframe[j,k] == 2
                new_frame[j,k] = 0.0; # sell
            end
        end
    end
    display(Gray.(new_frame[10:30,10:30]))
end





average_state_array = Array{Float64,2}(undef, number_of_iterations, number_of_colors)
for i ∈ 0:(number_of_iterations - 1)
    myframe = frames[i];
    for s ∈ 1:number_of_colors
        average_state_array[i+1,s] = findall(x -> x == s-1, myframe) |> length |> len -> (len/number_of_agents)
    end
end
average_state_array;








let
    q = plot(bg="gray95", background_color_outside="white", framestyle = :box, fg_legend = :transparent, legend=:topright);
    color_dictionary = Dict(0 =>:green,1=>:gray20,2=>:red);
    color_label_dictionary = Dict(0 => "white", 1 => "gray", 2 => "black");
    for s ∈ 1:number_of_colors
        plot!(q, average_state_array[:,s], label="state = $(s-1) ($(color_label_dictionary[s-1]))", 
            lw=2, c=color_dictionary[s-1], alpha=0.65)
    end
    current()
    xlabel!("Period (min)", fontsize=18)
    ylabel!("State fraction (AU)", fontsize=18)
end





let
    i,j = middle,middle; # center agent
    index_array = range(turn_index_to_view-view_width,stop=turn_index_to_view+view_width,step=1)
    
    agent_dynamics_array = Array{Int64,1}(undef, number_of_iterations)
    for k ∈ 0:(number_of_iterations - 1)
        frame = frames[k];
        agent_dynamics_array[k+1] = frame[i,j];
    end
    plot(agent_dynamics_array[index_array], ylims=(-1,3), linetype=:steppost, c=:black, lw=2, label="agent = ($(i),$(j))")
    scatter!(agent_dynamics_array[index_array], label="", c=:gray95)
    xlabel!("Period $(first(index_array)) → $(last(index_array)) (min)", fontsize=18)
    ylabel!("Agent state (AU)", fontsize=18)
end








î,ĵ = middle,middle
period =  range(turn_index_to_view - view_width, stop=turn_index_to_view+view_width,step=1) |> collect;





moves_dictionary = Dict(
    1 => (-1,0), # up
    2 => (1,0),  # down
    3 => (0,-1), # left
    4 => (0,1),  # right
);





in_sample_dataset = Dict{Int64, Array{Int64,1}}();
N = length(period);
for p ∈ 1:(N - 1)  

    my_current_frame = period[p] |> t-> frames[t];
    my_next_frame = period[p+1] |> t-> frames[t];
    
    data = Array{Int64,1}(undef, radius+1)
    for k ∈ 1:radius
        Δ = moves_dictionary[k];
        i,j = î + Δ[1], ĵ + Δ[2];        
        data[k] = my_current_frame[i,j];
    end
    data[radius + 1] = my_next_frame[î,ĵ]; # reminder: the rule gives us the *next* state    
    in_sample_dataset[p] = data; 
end


length(in_sample_dataset)





let
    Q = policy_model.Q;
    R = policy_model.rule;
    N = length(period);
    for p ∈ 1:(N - 1)
        data = in_sample_dataset[p];
        j = Q[round(mean(data[1:radius]), digits=2)]; # index in rule
        s = R[j]; # actual *next* state s
        ŝ = data[end]; # predicted *next* state ŝ
    
        # Are the actual and predicted *next* states the same?
        @assert s == ŝ
    end
end








worlfram_gridworld_model = build(MyWolframGridWorldModel, (
    number_of_states = length(policy_model.rule),
    data = in_sample_dataset,
    policymap = policy_model.Q
));





learning_model = build(MyWolframRuleQLearningAgentModel, (
    states = range(1, stop=number_of_states, step=1) |> collect,
    actions = range(1, stop=number_of_colors, step=1) |> collect,
    γ = 0.95,
    α = 0.70,
    Q = zeros(number_of_states,number_of_colors),
));





test = VLQuantitativeFinancePackage.sample(learning_model, worlfram_gridworld_model, 
    maxsteps = length(in_sample_dataset) - 1);


Q̂ = test.Q





π̂ = Dict{Int64,Int64}()
for s ∈ 0:(number_of_states - 1)
    π̂[s] = argmax(Q̂[s+1,:]) - 1;
end
π̂


policy_model.rule





R = policy_model.rule;
for s ∈ 0:(number_of_states - 1)
    
    true_action = R[s];      # get the true action from the rule for state s
    estimated_action = π̂[s]; # get the action we estimated for state s

    if (true_action != estimated_action)
        println("Error: for state $(s) true action is $(true_action) we estimated $(estimated_action)")
        @assert false;
    end
end











# for i ∈ 0:(number_of_iterations - 1)
#     frame = frames[i];
#     new_frame = Array{Float64,2}(undef, number_of_rows, number_of_columns);
#     for j ∈ 1:number_of_rows
#         for k ∈ 1:number_of_columns
#             if frame[j,k] == 0
#                 new_frame[j,k] = 1.0;
#             elseif frame[j,k] == 1
#                 new_frame[j,k] = 0.5;
#             elseif frame[j,k] == 2
#                 new_frame[j,k] = 0.0;
#             end
#         end
#     end
#     display(Gray.(new_frame))
#     IJulia.clear_output(true)
# end



