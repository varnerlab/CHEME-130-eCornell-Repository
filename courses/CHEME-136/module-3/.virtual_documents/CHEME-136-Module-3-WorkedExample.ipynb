





include("Include.jl");





rule_index = 6735; # the Wolfram rule index, see below for the range of possible values
number_of_colors = 3; # each the number of colors, i.e., the number of possible states for each agent. 
radius = 4; # size of the neighborhood an agent looks at. DO NOT CHANGE
number_of_rows = 2^9; # number of rows in the 2D agent grid. Default: 512. Larger values will **slow down** the computation below
number_of_columns = 2^9; # number of cols in the 2D agent grid. Default: 512. Larger values will **slow down** the computation below
number_of_agents = number_of_rows*number_of_columns; # the number of agents in the grid
middle = (number_of_rows / 2) |> Int; # the index of the middle agent (we have a square grid by default, so middle x = middle y)
number_of_iterations = 2^10; # the number of turns we simulate the system for. Default: 1024 turns
agent_view_width = 15; # ± this value from a center point will be used for plotting
agent_train_width = 50; # ± this value from a center point will be used for training





î,ĵ = 10,middle # agent location to view. This can be any value î ∈ 2:number_of_rows-1 and ĵ ∈ 2:number_of_cols-1
turn_index_to_view = 600; # turn to view in plots and training
view_period =  range(turn_index_to_view - agent_view_width, stop=turn_index_to_view+agent_view_width,step=1) |> collect; # plot range
train_period = range(turn_index_to_view - agent_train_width, stop=turn_index_to_view+agent_train_width,step=1) |> collect; # train range





number_of_states = range(0, stop = (number_of_colors - 1), step = (1/radius)) |> length;
total_number_policies = [(number_of_colors - 1)*(number_of_colors^(i-1)) for i ∈ 1:number_of_states] |> sum
println("For $(number_of_colors) colors with a radius = $(radius), the number of totalistic rules: $(total_number_policies + 1)")





policy_model = build(MyTwoDimensionalTotalisticWolframRuleModel, (
    index = rule_index,
    colors = number_of_colors,
    radius = radius,
));





number_of_states = policy_model.rule |> length
policy_model.rule





policy_model.neighborhoodstatesmap











# build an initial array - the initial state of the world 
initialstate = Array{Int64,2}(undef, number_of_rows, number_of_columns);
for row ∈ 1:number_of_rows
    for column ∈ 1:number_of_columns
        if (row == 1 && column == number_of_columns)
            initialstate[row, column] = 0;
        elseif (row == middle && column == middle)
            initialstate[row, column] = 0;
        elseif (row == number_of_rows && column == 1)
            initialstate[row, column] = 0;
        elseif (row == number_of_rows && column == number_of_columns)
            initialstate[row, column] = 0;
        elseif (row == 1 && column == 1)
            initialstate[row, column] = 0;
        elseif (row == middle && column == 1)
            initialstate[row, column] = 0;
        elseif (row == 1 && column == middle)
            initialstate[row, column] = 0;
        elseif (row == middle && column == number_of_columns)
            initialstate[row, column] = 0;
        elseif (row == number_of_rows && column == middle)
            initialstate[row, column] = 0;
        else
            initialstate[row, column] = 1;
        end
    end
end





frames = solve(policy_model, initialstate, steps = number_of_iterations);





moves_dictionary = Dict(
    1 => (-1,0), # up
    2 => (1,0),  # down
    3 => (0,-1), # left
    4 => (0,1),  # right
);





in_sample_dataset = Dict{Int64, Array{Int64,1}}();
N = length(train_period);
for p ∈ 1:(N - 1)  

    my_current_frame = train_period[p] |> t-> frames[t];
    my_next_frame = train_period[p+1] |> t-> frames[t];
    
    data = Array{Int64,1}(undef, radius+1)
    for k ∈ 1:radius
        Δ = moves_dictionary[k];
        i,j = î + Δ[1], ĵ + Δ[2];        
        data[k] = my_current_frame[i,j];
    end
    data[radius + 1] = my_next_frame[î,ĵ]; # reminder: the rule gives us the *next* state    
    in_sample_dataset[p] = data; 
end





myframe = frames[turn_index_to_view]





my_color_dictionary = Dict{Int64, RGB}();
my_color_dictionary[0] = colorant"lightgreen"; # 0: buy state
my_color_dictionary[1] = colorant"white";      # 1: hold state
my_color_dictionary[2] = colorant"red";        # 2: sell state


let
    new_display_frame = Array{RGB,2}(undef, number_of_rows, number_of_columns);
    for j ∈ 1:number_of_rows
        for k ∈ 1:number_of_columns
            new_display_frame[j,k] = myframe[j,k] |> s -> my_color_dictionary[s];
        end
    end
    display(new_display_frame)
end





let
    Q = policy_model.neighborhoodstatesmap;
    R = policy_model.rule;
    N = length(train_period);
    for p ∈ 1:(N - 1)
        data = in_sample_dataset[p];
        j = Q[round(mean(data[1:radius]), digits=2)]; # index in rule
        s = R[j]; # actual *next* state s
        ŝ = data[end]; # predicted *next* state ŝ
    
        # Are the actual and simulated *next* states the same?
        @assert s == ŝ
    end
end





average_state_array = Array{Float64,2}(undef, number_of_iterations, number_of_colors)
for i ∈ 0:(number_of_iterations - 1)
    myframe = frames[i];
    for s ∈ 1:number_of_colors
        average_state_array[i+1,s] = findall(x -> x == s-1, myframe) |> length |> len -> (len/number_of_agents)
    end
end
average_state_array;





let
    q = plot(bg="gray95", background_color_outside="white", framestyle = :box, fg_legend = :transparent, legend=:topright);
    color_dictionary = Dict(0 =>:green,1=>:gray20,2=>:red);
    color_label_dictionary = Dict(0 => "buy", 1 => "hold", 2 => "sell");
    for s ∈ 1:number_of_colors
        plot!(q, average_state_array[:,s], label="state = $(s-1) ($(color_label_dictionary[s-1]))", 
            lw=2, c=color_dictionary[s-1], alpha=0.65)
    end
    current()
    xlabel!("Period (min)", fontsize=18)
    ylabel!("State fraction (AU)", fontsize=18)
end





let
    neighborhood_state_array = Array{Int64,1}();
    Q = policy_model.neighborhoodstatesmap;
    N = length(view_period);
    for p ∈ 1:(N - 1)
        data = in_sample_dataset[p];
        j = Q[round(mean(data[1:radius]), digits=2)]; # index in rule
        push!(neighborhood_state_array,j);
    end
    q = plot(bg="gray95", background_color_outside="white", framestyle = :box, fg_legend = :transparent, legend=:topright);
    plot(q, neighborhood_state_array, linetype=:steppost, lw=3, c=:deepskyblue3, label="agent = ($(î),$(ĵ))")
    scatter!(neighborhood_state_array, label="", c=:white, msc=:deepskyblue3)
    xlabel!("Period $(first(view_period)) → $(last(view_period)) (min)", fontsize=18)
    ylabel!("Agent input state (AU)", fontsize=18)
end





let

    neighborhood_state_array = Array{Int64,1}();
    Q = policy_model.neighborhoodstatesmap;
    N = length(train_period);
    for p ∈ 1:(N - 1)
        data = in_sample_dataset[p];
        j = Q[round(mean(data[1:radius]), digits=2)]; # index in rule
        push!(neighborhood_state_array,j);
    end

    neighborhood_freq_dict = Dict{Int64,Float64}()
    for s ∈ 0:(number_of_states - 1)
        neighborhood_freq_dict[s] = findall(x-> x == s, neighborhood_state_array) |> length |> x-> x/(N-1)
    end

    table_df = DataFrame()
    for s ∈ 0:(number_of_states - 1)
        row_data = (
            state = s,
            frequency = neighborhood_freq_dict[s]
        );
        push!(table_df, row_data);
    end
    pretty_table(table_df)
end





let
    agent_dynamics_array = Array{Int64,1}(undef, number_of_iterations)
    for k ∈ 0:(number_of_iterations - 1)
        frame = frames[k];
        agent_dynamics_array[k+1] = frame[î,ĵ];
    end
    
    q = plot(bg="gray95", background_color_outside="white", framestyle = :box, fg_legend = :transparent, legend=:topright);
    plot(q, agent_dynamics_array[view_period], ylims=(-1,3), linetype=:steppost, c=:darkorange, lw=3, label="agent = ($(î),$(ĵ))")
    scatter!(agent_dynamics_array[view_period], label="", c=:white, msc=:darkorange)
    xlabel!("Period $(first(view_period)) → $(last(view_period)) (min)", fontsize=18)
    ylabel!("Agent output state (AU)", fontsize=18)
end











function myworld(model::MyWolframGridWorldModel, t::Int, s::Int, a::Int)::Tuple{Int, Float64}
    
    # initialize -
    s′ = nothing
    r = nothing
    
    # grab the parameters from the model -
    dataset = model.data;
    policymap = model.policymap;

    # what is the state, action and reward?
    data = dataset[t];
    a′ = data[end]+1; # the last element is the next state (correct for zero)
    if (a′ == a)
        r = 1.0;
    else
        r = -1.0;
    end

    # get the next state - this assumes we have full knowledge of the playback frames
    next_state_data = dataset[t+1];
    my_neighbors = next_state_data[1:end-1];
    s′ = round(mean(my_neighbors), digits=2) |> value -> policymap[value] + 1; # correct for zero

    # return -
    return (s′,r);
end;





worlfram_gridworld_model = build(MyWolframGridWorldModel, (
    number_of_states = length(policy_model.rule),
    data = in_sample_dataset,
    policymap = policy_model.neighborhoodstatesmap,
    world = myworld,
));





learning_model = build(MyWolframRuleQLearningAgentModel, (
    states = range(1, stop=number_of_states, step=1) |> collect,
    actions = range(1, stop=number_of_colors, step=1) |> collect,
    γ = 0.95,
    α = 0.70,
    Q = zeros(number_of_states,number_of_colors),
));





test = VLQuantitativeFinancePackage.sample(learning_model, worlfram_gridworld_model, 
    maxsteps = length(in_sample_dataset) - 1);





Q̂ = test.Q





π̂ = Dict{Int64,Int64}()
for s ∈ 0:(number_of_states - 1)
    π̂[s] = argmax(Q̂[s+1,:]) - 1;
end





let
    table_df = DataFrame();
    for s ∈ 0:(number_of_states - 1)

        a = policy_model.rule[s]; # true action for state s
        â = π̂[s]; # estimated action for state s
        
        row_df = (
            s = s,
            a = a,
            â = â,
            correct = (a==â)
        );
        push!(table_df, row_df);
    end
    pretty_table(table_df)
end





# for i ∈ 0:(number_of_iterations - 1)
#     myframe = frames[i];
#     new_display_frame = Array{RGB,2}(undef, number_of_rows, number_of_columns);
#     for j ∈ 1:number_of_rows
#         for k ∈ 1:number_of_columns
#             new_display_frame[j,k] = myframe[j,k] |> s -> my_color_dictionary[s];
#         end
#     end
#     display(new_display_frame)
#     IJulia.clear_output(true)
# end



