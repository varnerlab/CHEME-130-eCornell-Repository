





include("Include.jl");








share_price_data_df = CSV.read(joinpath(_PATH_TO_DATA, "QQQ-OHLC-1-min-aggregate-2024.csv"), DataFrame);





vwap_array = vwap(share_price_data_df);





share_price_data_df[:,:vwap] = vwap_array; # now the dataframe has the vwap data





number_of_trading_periods = length(vwap_array); # how many minutes of 2024 data do we have?
start_index = 60;
view_range = range(start_index,stop = 1560, step = 1) |> collect;





number_of_colors = 3; # each the number of colors, i.e., the number of possible states for each agent. 
radius = 4; # size of the neighborhood an agent looks at. DO NOT CHANGE
number_of_rows = 2^3; # number of rows in the 2D agent grid. Default: 512. Larger values will **slow down** the computation below
number_of_columns = 2^3; # number of cols in the 2D agent grid. Default: 512. Larger values will **slow down** the computation below
number_of_agents = number_of_rows*number_of_columns; # the number of agents in the grid
middle = (number_of_rows / 2) |> Int; # the index of the middle agent (we have a square grid by default, so middle x = middle y)
number_of_iterations = 2^10; # the number of turns we simulate the system for. Default: 1024 turns





number_of_edge_cells = 2*(number_of_rows + number_of_columns) - 4;





edge_coordinate_dictionary = Dict{Int64, Tuple{Int,Int64}}();
counter = 1;
for row ∈ 1:number_of_rows
    for col ∈ 1:number_of_columns

        if (row == 1)
            edge_coordinate_dictionary[counter] = (row,col);
            counter += 1;
        elseif (row == number_of_rows)
            edge_coordinate_dictionary[counter] = (row,col);
            counter += 1;
        elseif (col == 1)
            edge_coordinate_dictionary[counter] = (row,col);
            counter += 1;
        elseif (col == number_of_columns)
            edge_coordinate_dictionary[counter] = (row,col);
            counter += 1;
        end
    end
end
edge_coordinate_dictionary





α = (1/60); # students can update this! what happens if we recompute the mvap_array with diff values of α?





mvwap_array = Array{Float64,1}(undef, number_of_trading_periods);
mvwap_array[1] = vwap_array[1]; 

for i ∈ 2:number_of_trading_periods
    previous_mvwap = mvwap_array[i-1];
    current_vwap = vwap_array[i];
    mvwap_array[i] = previous_mvwap + α*(current_vwap - previous_mvwap);
end





let
    q = plot(bg="gray95", background_color_outside="white", framestyle = :box, fg_legend = :transparent, legend=:topright);
    plot(q, vwap_array[view_range],c=:blue, lw=3, label="QQQ VWAP")
    plot!(mvwap_array[view_range], c=:red, lw=3, label="QQQ MWVAP (α = $(round(α, digits=4)))")
    xlabel!("Trading period index $(first(view_range)) → $(last(view_range)) (min)", fontsize=18)
    ylabel!("Share Price (USD/share)", fontsize= 18);
end








decision_boundary_array = Array{Float64,1}(undef, number_of_trading_periods);
decision_boundary_array[1] = 0.0;
for i ∈ 2:(number_of_trading_periods)
    previous_decision_boundary = decision_boundary_array[i-1];
    ϵ = (vwap_array[i] - mvwap_array[i])^2 |> x -> sqrt(x);
    decision_boundary_array[i] = previous_decision_boundary + α*(ϵ - previous_decision_boundary);
end





β = 1.0;





let    
    q = plot(bg="gray95", background_color_outside="white", framestyle = :box, fg_legend = :transparent, legend=:topright);
    plot(q, vwap_array[view_range],c=:blue, lw=2, label = "QQQ VWAP")

    UB = mvwap_array[view_range] + β*decision_boundary_array[view_range];
    LB = mvwap_array[view_range] - β*decision_boundary_array[view_range];
    plot!(mvwap_array[view_range], fillrange = UB, label="", alpha=0.25, c=:gray20)
    plot!(mvwap_array[view_range], fillrange = LB, label="", alpha=0.25, c=:gray20)
    plot!(mvwap_array[view_range], c=:red, lw = 3, label="QQQ MWVAP (α = $(round(α, digits=4)))")
    xlabel!("Trading period index ($(first(view_range)) → $(last(view_range)) min)", fontsize=18)
    ylabel!("Share Price (USD/share)", fontsize= 18);
end








number_of_colors = 3; # the number of colors, i.e., the number of possible states for each agent. 
radius = 4; # size of the neighborhood an agent looks at. DO NOT CHANGE





neighborhood_configuration_array = range(0, stop = (number_of_colors - 1), step = (1/radius)) |> collect





@assert length(neighborhood_configuration_array) == (number_of_colors - 1)*radius + 1





internal_agent_policy = Dict{Int64, Int64}()
internal_agent_policy[0] = 0; # c: 0 => 0.00; 4/4 of my neighbors are in the buy state => I should buy = state 0
internal_agent_policy[1] = 0; # c: 1 => 0.25; 3/4 of my neighbors are in the buy state, 1 neighbor is holding => I'll buy = state 0
internal_agent_policy[2] = 0; # c: 2 => 0.50; 2/4 of my neighbors are in the buy state, 2 neighbors are holding => I'll buy = state 0 # agressive
internal_agent_policy[3] = 1; # c: 3 => 0.75; 1/4 of my neighbors are in the buy state, 3 neighbors are holding => I'll hold = state 1
internal_agent_policy[4] = 1; # c: 4 => 1.00; 4/4 of my neighbors are holding => I'll hold = state 1
internal_agent_policy[5] = 1; # c: 5 => 1.25; 3/4 of my neighbors are holding, 1 neighbor is selling => I'll hold = state 1
internal_agent_policy[6] = 1; # c: 6 => 1.50; 2/4 of my neighbors are holding, 2 neighbors are selling => I'll hold = state 1
internal_agent_policy[7] = 2; # c: 7 => 1.75; 1/4 of my neighbors are holding, 3 neighbors are selling => I'll sell = state 2
internal_agent_policy[8] = 2; # c: 8 => 2.00; 4/4 of my neighbors are selling  => I'll sell = state 2





number_of_configurations = (number_of_colors - 1)*radius + 1;
rule_index = [internal_agent_policy[i]*(number_of_colors^(i)) for i ∈ 0:(number_of_configurations - 1)] |> sum
println("The rule_index for our *expert* manual policy = $(rule_index)")





policy_model = build(MyTwoDimensionalTotalisticWolframRuleModel, (
    index = rule_index,
    colors = number_of_colors,
    radius = radius,
));





R = policy_model.rule
for (k,v) ∈ internal_agent_policy
    @assert v == R[k]
end











initialstate = Array{Int64,2}(undef, number_of_rows, number_of_columns) |> X -> fill!(X,1); # nice ...





edge_cell_next_state_dictionary = Dict{Int64, Array{Int64,1}}();
for i ∈ eachindex(view_range)
    t = view_range[i];

    # setup decision range -
    UB = mvwap_array[t] + β*decision_boundary_array[t];
    LB = mvwap_array[t] - β*decision_boundary_array[t];
    Sᵢ = vwap_array[t];

    next_state = nothing;
    if (LB ≤ Sᵢ ≤ UB)
        next_state = 1; # We are in the gray region => hold
    elseif (Sᵢ > UB)
        next_state = 2; # We are beyond the upper bound => sell
    else
        next_state = 0; # We are below the lower bound => buy
    end
    
    # iterate through each edge cell and update its next state value -
    next_state_array = Array{Int64,1}(undef, number_of_edge_cells);
    for i ∈ 1:number_of_edge_cells
        next_state_array[i] = next_state;
    end
    edge_cell_next_state_dictionary[t] = next_state_array
end
edge_cell_next_state_dictionary





tmp = Dict{Int64,Array{Int64,2}}();
previous_state_array = copy(initialstate);
for i ∈ eachindex(view_range)
    t = view_range[i];

    # update edge cells in the previous state -
    edge_cell_state_array = edge_cell_next_state_dictionary[t];
    for j ∈ 1:number_of_edge_cells
        coordinate = edge_coordinate_dictionary[j];
        previous_state_array[coordinate[1],coordinate[2]] = edge_cell_state_array[j];
    end

    # run the simulated 1-turn
    frames = solve(policy_model, previous_state_array, steps = 2);
    
    # get the frame -
    previous_state_array = copy(frames[1]);
    tmp[i] = frames[1];
end


my_color_dictionary = Dict{Int64, RGB}();
my_color_dictionary[0] = colorant"lightgreen"; # 0: buy state
my_color_dictionary[1] = colorant"gray60";      # 1: hold state
my_color_dictionary[2] = colorant"red";        # 2: sell state


let
    myframe = tmp[100]
    new_display_frame = Array{RGB,2}(undef, number_of_rows, number_of_columns);
    for j ∈ 1:number_of_rows
        for k ∈ 1:number_of_columns
            new_display_frame[j,k] = myframe[j,k] |> s -> my_color_dictionary[s];
        end
    end
    display(new_display_frame)
end



